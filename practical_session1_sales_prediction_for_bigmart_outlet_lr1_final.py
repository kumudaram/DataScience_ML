# -*- coding: utf-8 -*-
"""Practical_Session1_Sales Prediction for Bigmart Outlet_LR1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v8R2QGNKRy3wbCYwX_qIDIrvI-lioR6Z

**Sales Prediction for Bigmart Outlet**

Data: Collected from 2013
Files: train, test and submission
Train: 8523 records
Test: 5681 records

Type of Problem: Supervised Learning(as we need to predict)

Target Column: Item_outlet_sales

**Understand More about the data**
"""

#Import Lib

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import warnings
warnings.filterwarnings('ignore')

train = pd.read_csv('/content/train_v9rqX0R.csv')

#taking backup
train_backup= train

train.shape

test = pd.read_csv('/content/test_AbJTz2l.csv')

test.shape

train.columns

test.columns

#Identify difference bet train and test columns(comparing coulmn names)

train.columns.difference(test.columns)

train.head()

test.head()

"""**Basic info about Data set:**

**Based on Outlevel data:**
Size of the outlet-yes,
age(year of opening)-yes,
Location-yes,
Outlet type-yes.

**Based on product:**
price-yes,
weight-yes,
quality-no,
quantity-yes,
advertisement-yes(item_visibility),
item_type-yes.

"""

train.dtypes

train.info()

"""**Descriptive Analytics:**"""

train.columns

train.isna().sum()

train.duplicated().sum()

train.head()

train['Item_Fat_Content'].nunique()

train.nunique()

train['Item_Fat_Content'].unique()

columns_with_less_unique = ['Item_Fat_Content','Item_Type', 'Outlet_Identifier', 'Outlet_Establishment_Year', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']

#unique value of each columns
for col in columns_with_less_unique:
    print(col, '=', train[col].nunique(), train[col].unique())

"""Item_Fat_Content contains repeated info with different names(replace) Low Fat/low fat/LF

Item_Type: categorise as 1. Drinks(soft_drinks, harddrinks etc..), 2. Foods(starchy foods, snack foods, frozen foods etc..), 3. others

Total outlets=10
OutletType=4
OutletLocation=3
outlet Size=3
"""

train.isna().sum()

"""# Contains Null

Item_weight
Outlet_size
"""

train['Item_Weight'].mean()

train[train['Item_Weight'].isna()].head(2)

train[train['Item_Identifier']=='FDP10']['Item_Weight']

train[train['Item_Identifier']=='FDP10']['Item_Weight'].mean()

train[train['Item_Identifier']=='DRI11']['Item_Weight']

train[train['Item_Identifier']=='DRI11']['Item_Weight'].mean()

train.groupby('Item_Identifier').Item_Weight.mean()

#will take mean of Item wieght

train['Item_Weight_new1']=train.Item_Weight.mean()

#will check on each identifier and get mean of it and fill it with NAN based on identifier's mean value
#transform- will update the new value in place of nan
train['Item_Weight_new2']=train.groupby('Item_Identifier').Item_Weight.transform(lambda x:x.fillna(x.mean()))

#displaying the diff bet 2 mean val identified
train[['Item_Identifier', 'Item_Weight', 'Item_Weight_new1', 'Item_Weight_new2']].head(2)

train[train['Item_Weight'].isna()].head(2)

"""#EDA
#Univariate Analysis
"""

train.columns

train.Item_Outlet_Sales

sns.displot(train['Item_Outlet_Sales'])

train['Item_Outlet_Sales'].hist()

train['Item_Outlet_Sales'].hist(bins=6000)

"""#sales are high with  2000. Sales are very less above 8000"""

sns.displot(train['Item_Weight'])

sns.displot(train['Item_Visibility'])

"""#Lot of items are not showcased enough. when visibility is high they are putting lesser values"""

#price with range 100-200 have more sales.
sns.displot(train['Item_MRP'])

train['Outlet_Establishment_Year'].value_counts()

#no shops during year 1988-1995
sns.displot(train['Outlet_Establishment_Year'])

train.columns

numerical_columns = train.select_dtypes(include=np.number).columns

#item_visibility outliers are there which ranges vet 2.0-3.0-we can ignore by keeping outliers
#boxplot lower /upper bound, IQR
for col in numerical_columns:
    sns.boxplot(data=train, x=col)
    plt.show()

numerical_columns = train.select_dtypes(include=np.number).drop(columns=['Item_Outlet_Sales']).columns

#excluded item_weight_new1/Item_weight_new2
#fig's size-20-height, 8-width, fig-container for each boxplot(variable name)
fig, ax= plt.subplots(nrows=1,ncols=4, figsize=(20,8))

for i, col in enumerate(numerical_columns):
    sns.boxplot(data=train, y=col, ax=ax[i])
    ax[i].set_title(col)
plt.show()

category_columns = train.select_dtypes(exclude=np.number).columns

category_columns

#Category plots
sns.countplot(data=train, x='Item_Fat_Content')

sns.countplot(data=train, y='Item_Type')

sns.countplot(data=train, x='Outlet_Size')

#%.2f%%- ercentage format in decimal value 2 digit
train['Outlet_Size'].value_counts().plot(kind='pie', autopct='%.2f%%')

train['Outlet_Size'].value_counts()

sns.scatterplot(data=train, x='Item_Weight', y='Item_Outlet_Sales')

sns.scatterplot(data=train, x='Item_Visibility', y='Item_Outlet_Sales', hue='Outlet_Type')

train.columns

#correlation of numerical col with target out val to check the correlation
#axis=0, default by row
#mrp is the highly correlated only
sns.heatmap(pd.concat([train[numerical_columns],train['Item_Outlet_Sales']], axis=1).corr(), annot=True)

train['age_of_outlet']=2024-train['Outlet_Establishment_Year']

train.head(2)

#drop establishment of years and include age of outlet
!python --version

#base line model

train = pd.read_csv('/content/train_v9rqX0R.csv')

submission = pd.read_csv('/content/sample_submission_8RXa3c6.csv')

submission.columns

#Mean taken from historical data
mean_sales = train['Item_Outlet_Sales'].mean()

#define a dataframe for submission
base_model = test[['Item_Identifier', 'Outlet_Identifier']]
base_model['Item_Outlet_Sales'] = mean_sales
base_model

base_model.to_csv('submission_by_base_modelling.csv', index=False)

"""RMSE Score:
**Your score for this submission is : 1773.8251377790564.**
"""

train.isna().sum()

train['Item_Weight'].mean()

train['Item_Weight'].fillna(train['Item_Weight'].mean(), inplace=True)

train.isna().sum()

train['Outlet_Size'].mode().values[0]

train['Outlet_Size'].fillna(train['Outlet_Size'].mode().values[0], inplace=True)

train.isna().sum()

train.duplicated().sum()



cat_cols = train.select_dtypes(include='object').drop(columns='Item_Identifier').columns
cat_cols

#num_cols = train.select_dtypes(include='object').drop(columns='Item_Outlet_Sales').columns
num_cols = train.select_dtypes(exclude='object').drop(columns='Item_Outlet_Sales').columns
num_cols

train[cat_cols].head(2)

train[num_cols].head(2)

#convert Category into columns
preprocess_category_train_columns =pd.get_dummies(train[cat_cols],dtype=int)
preprocess_category_train_columns.head(2)

preprocess_category_train_columns.columns

train[cat_cols].nunique()

train[num_cols].head(2)

#feature scailing to change in smimilar values since some data in decimal, some in hundeds, some in thousand

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()



preprocess_numeric_train_columns =pd.DataFrame(scaler.fit_transform(train[num_cols]), columns =num_cols)
preprocess_numeric_train_columns.head(2)

train_preprocessed = pd.concat([preprocess_numeric_train_columns, preprocess_category_train_columns, train['Item_Outlet_Sales']], axis=1)
train_preprocessed.head(2)

x = train_preprocessed.drop(columns=['Item_Outlet_Sales'])
y = train_preprocessed['Item_Outlet_Sales']

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

train_x, val_x, train_y, val_y = train_test_split(x,y,test_size=0.2, random_state=42)
train.shape, train_x.shape, train_y.shape, val_x.shape, val_y.shape

model = LinearRegression()

model.fit(train_x, train_y)

pred_train_y= model.predict(train_x)

from sklearn.metrics import mean_squared_error

print('Train RMSE',mean_squared_error(train_y, pred_train_y, squared=False))

pred_val_y = model.predict(val_x)
print('Validation RMSE',mean_squared_error(val_y, pred_val_y, squared=False))

test.columns

#pipeline to fit and predict
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.feature_selection import SelectKBest, f_regression

train = pd.read_csv('train_v9rqX0R.csv')
category_columns_to_encode = train.select_dtypes(include='object').drop(columns='Item_Identifier').columns
category_columns_to_encode
numeric_columns_to_encode = train.select_dtypes(exclude='object').drop(columns='Item_Outlet_Sales').columns
numeric_columns_to_encode

cat_pipe_encode =Pipeline(steps = [('impute_cat', SimpleImputer(strategy='most_frequent')), ('ohe', OneHotEncoder())])

num_pipe_encode =Pipeline(steps = [('impute_num', SimpleImputer(strategy='mean')), ('scale', StandardScaler())])

preprocess = ColumnTransformer(transformers=[('num_encode',num_pipe_encode, numeric_columns_to_encode),('cat_encode', cat_pipe_encode, category_columns_to_encode)])

model_pipeline = Pipeline(steps=[('preprocessing',preprocess),
 #('feat_select', SelectKBest(f_regression, k=6)),
  ('lin_reg', LinearRegression())
  ])

x = train.drop(columns=['Item_Outlet_Sales'])
y= train[['Item_Outlet_Sales']]

x.head(2)

y.head(2)

train_x,val_x,train_y,val_y =train_test_split(x,y, test_size=0.2, random_state=42)
train_x.shape,val_x.shape,train_y.shape,val_y.shape

model_pipeline.fit(train_x,train_y)

pred_train_y = model_pipeline.predict(train_x)
print('Train RMSE',mean_squared_error(train_y, pred_train_y, squared=False))

pred_val_y = model_pipeline.predict(val_x)
print('Validation RMSE',mean_squared_error(val_y, pred_val_y, squared=False))

y_sub = model_pipeline.predict(test)

submission.head(2)

#to check on negative values in pred target
submission[submission['Item_Outlet_Sales']<0]

submission['Item_Outlet_Sales']=y_sub
submission.to_csv('sub_using_pipeline1.csv', index=False)

submission.head(5)

#assigning negative value to 0
submission.loc[submission['Item_Outlet_Sales']<0, 'Item_Outlet_Sales'] = 0

#combining to code and exporting in csv
submission['Item_Outlet_Sales']=y_sub
submission.loc[submission['Item_Outlet_Sales']<0, 'Item_Outlet_Sales'] = 0
submission.to_csv('sub_using_pipeline2.csv', index=False)

#checking for negative value and returns 0
submission[submission['Item_Outlet_Sales']<0]